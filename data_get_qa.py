import json
import json5
import os
import re
import time
import logging
import asyncio
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm
from openai import OpenAI

logging.basicConfig(level=logging.INFO)

# ========== é…ç½® ========== #
INPUT_DIR = ""  # è¾“å…¥çš„ txt æˆ– json æ–‡ä»¶ç›®å½•
OUTPUT_DIR = ""  # ç»“æœè¾“å‡ºç›®å½•
PROMPT_TEMPLATE_PATH = ""  # Prompt æ¨¡æ¿è·¯å¾„
MODEL_NAME = "deepseek-v3"  # ä½¿ç”¨çš„ LLM æ¨¡å‹
MAX_LINES_PER_FILE = 500  # æ‹†åˆ†æ–‡æœ¬çš„è¡Œæ•°é™åˆ¶
MAX_CONCURRENT_TASKS = 10  # æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ========== åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ ========== #
model_client_map = {
    MODEL_NAME: OpenAI(
        api_key="",
        base_url="https://api.zhizengzeng.com/v1",
    )
}

# ========== è¯»å–å¹¶è§£æ JSON æ–‡ä»¶ ========== #
def read_json_file(file_path):
    """è¯»å– JSON æ–‡ä»¶å¹¶è¿”å›å†…å®¹"""
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            return json.load(file)
    except Exception as e:
        print(f"âŒ æ— æ³•è¯»å– JSON æ–‡ä»¶ {file_path}ï¼Œé”™è¯¯: {e}")
        return None

# ========== æŒ‰è¡Œæ•°æ‹†åˆ†æ–‡æœ¬æ–‡ä»¶å‡½æ•°ï¼ˆå¹¶å¸¦ç›‘æ§è¿›åº¦ï¼‰ ========== #
def split_txt_file(file_path, max_lines=MAX_LINES_PER_FILE):
    base_name, ext = os.path.splitext(os.path.basename(file_path))

    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            lines = file.readlines()
    except Exception as e:
        print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶ {file_path}ï¼Œé”™è¯¯: {e}")
        return []

    total_lines = len(lines)

    if total_lines <= max_lines:
        print(f"ğŸ“„ æ–‡ä»¶ {file_path} å†…å®¹å°äº {max_lines} è¡Œï¼Œæ— éœ€æ‹†åˆ†")
        return [''.join(lines)]

    num_parts = (total_lines // max_lines) + 1

    split_contents = []
    for i in range(num_parts):
        start_index = i * max_lines
        end_index = min((i + 1) * max_lines, total_lines)
        part_content = ''.join(lines[start_index:end_index])
        split_contents.append(part_content)

    return split_contents

# ========== è¯»å– Prompt æ¨¡æ¿ ========== #
def read_prompt_template():
    with open(PROMPT_TEMPLATE_PATH, "r", encoding="utf-8") as f:
        return f.read()

# ========== è¯»å–å¹¶æ‹†åˆ†è¾“å…¥ç›®å½•ä¸­çš„ txt å’Œ json æ–‡ä»¶ï¼ˆå¹¶å¸¦è¿›åº¦ç›‘æ§ï¼‰ ========== #
def read_and_split_files():
    files = [f for f in os.listdir(INPUT_DIR) if f.endswith(".txt") or f.endswith(".json")]
    file_contents = {}

    # ä½¿ç”¨å¹¶å‘æ–¹å¼å¤„ç†æ–‡ä»¶æ‹†åˆ†
    with ThreadPoolExecutor() as executor:
        futures = {
            file: executor.submit(process_file, os.path.join(INPUT_DIR, file))
            for file in files
        }

        # ä½¿ç”¨ tqdm è¿›åº¦æ¡ç›‘æ§æ‹†åˆ†è¿›åº¦
        for file, future in tqdm(futures.items(), desc="å¤„ç†æ–‡ä»¶ä¸­", total=len(futures)):
            split_contents = future.result()
            base_name = os.path.splitext(file)[0]  # å»æ‰æ–‡ä»¶æ‰©å±•å
            for idx, content in enumerate(split_contents):
                new_filename = f"{base_name}_{idx + 1}.txt"  # æ–°æ–‡ä»¶åï¼šåŸæ–‡æ¡£å_åºå·.txt
                file_contents[new_filename] = content

    return file_contents

def process_file(file_path):
    """å¤„ç†æ–‡ä»¶ï¼ˆtxt æˆ– jsonï¼‰"""
    ext = os.path.splitext(file_path)[1].lower()

    if ext == ".txt":
        return split_txt_file(file_path, MAX_LINES_PER_FILE)
    elif ext == ".json":
        content = read_json_file(file_path)
        if content:
            return [json.dumps(content, ensure_ascii=False)]  # å°† JSON å†…å®¹è½¬ä¸ºå­—ç¬¦ä¸²
        else:
            return []
    return []

# ========== æ£€æŸ¥è¾“å‡ºç›®å½•æ˜¯å¦å·²æœ‰ç»“æœï¼ˆå¦‚æœå·²å­˜åœ¨ä¸”éç©ºï¼Œåˆ™è·³è¿‡ï¼‰ ========== #
def check_if_result_exists(filename):
    output_path = os.path.join(OUTPUT_DIR, f"{os.path.splitext(filename)[0]}.json")

    if os.path.exists(output_path):
        with open(output_path, "r", encoding="utf-8") as f:
            try:
                result = json.load(f)
                if result == {}:  # å¦‚æœç»“æœæ˜¯ç©ºå­—å…¸ï¼Œç»§ç»­å¤„ç†
                    print(f"ğŸ“ `{filename}` ç»“æœä¸ºç©ºï¼Œç»§ç»­å¤„ç†ã€‚")
                    return False
                elif result:  # å¦‚æœç»“æœéç©ºï¼Œåˆ™è·³è¿‡
                    print(f"ğŸ“ `{filename}` å·²æœ‰æœ‰æ•ˆç»“æœï¼Œè·³è¿‡å¤„ç†ã€‚")
                    return True
            except json.JSONDecodeError:
                pass  # å¦‚æœæ— æ³•è§£æ JSONï¼Œè·³è¿‡

    return False

# ========== ç”Ÿæˆ LLM æ¶ˆæ¯æ ¼å¼ï¼ˆå¹¶å‘å¤„ç†ï¼‰ ========== #
async def generate_message_for_file(filename, content, prompt_template, progress_bar, semaphore):
    async with semaphore:
        # æ›´æ–°ä¸ºæ–°çš„ JSON æ–‡ä»¶è·¯å¾„
        json_filename = os.path.splitext(filename)[0] + ".json"
        json_path = os.path.join(OUTPUT_DIR, json_filename)

        # æ›¿æ¢ <txt> éƒ¨åˆ†å¹¶ç”Ÿæˆå®Œæ•´çš„ prompt
        prompt = prompt_template.replace("<txt>", content)

        progress_bar.update(1)  # æ›´æ–°è¿›åº¦æ¡
        return filename, [{"role": "user", "content": prompt}]


async def generate_messages_concurrently(prompt_template, txt_contents, semaphore):
    tasks = []
    with tqdm(total=len(txt_contents), desc="ç”Ÿæˆæ¶ˆæ¯ä¸­") as progress_bar:
        for filename, content in txt_contents.items():
            tasks.append(generate_message_for_file(filename, content, prompt_template, progress_bar, semaphore))

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        messages = await asyncio.gather(*tasks)

    # å°†ç»“æœè½¬æ¢æˆå­—å…¸å½¢å¼
    messages_dict = {filename: message for filename, message in messages}

    return messages_dict

# ========== æ¸…ç†å¹¶è§£æ LLM è¿”å›ç»“æœï¼ˆæ”¯æŒå¹¶å‘ï¼‰ ========== #
async def clean_and_parse_llm_result(raw_result: str):
    """ æ¸…ç† LLM ç”Ÿæˆçš„ JSON ç»“æœï¼Œå¹¶è§£æä¸º Python å­—å…¸ """

    if not raw_result.strip():
        logging.warning("âš ï¸ LLM ç»“æœä¸ºç©ºï¼")
        return {}

    # ç§»é™¤ Markdown ä»£ç å— ```json ... ```
    raw_result = re.sub(r"^```json\s*|\s*```$", "", raw_result.strip())

    try:
        # ä¼˜å…ˆä½¿ç”¨æ ‡å‡† JSON è§£æ
        return json.loads(raw_result)
    except json.JSONDecodeError as e:
        logging.warning(f"âš ï¸ æ ‡å‡† JSON è§£æå¤±è´¥: {e}")

    try:
        # å°è¯•ä½¿ç”¨ json5 è§£æï¼ˆæ”¯æŒæ›´å®½æ¾çš„ JSON è¯­æ³•ï¼‰
        return json5.loads(raw_result)
    except Exception as e:
        logging.error(f"âŒ JSON5 è§£æä»ç„¶å¤±è´¥: {e}")
        return {}

# ========== å¼‚æ­¥è°ƒç”¨å•ä¸ª LLM å¹¶å®æ—¶ä¿å­˜ç»“æœ ========== #
async def call_single_llm_json_async(task_id, filename, messages, model, progress_bar, semaphore):
    async with semaphore:
        max_retries = 3
        retry_count = 0

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²æœ‰ç»“æœï¼Œå¦‚æœå·²æœ‰ç»“æœåˆ™è·³è¿‡
        if check_if_result_exists(filename):
            progress_bar.update(1)  # è·³è¿‡çš„æ–‡ä»¶æ›´æ–°è¿›åº¦æ¡
            return filename, ""  # è·³è¿‡å½“å‰æ–‡ä»¶

        print(f"ğŸŸ¡ ä»»åŠ¡ {task_id}: å¼€å§‹è°ƒç”¨ {model} å¤„ç† `{filename}`...")

        while retry_count < max_retries:
            try:
                response = await asyncio.to_thread(
                    model_client_map[model].chat.completions.create,
                    model=model,
                    messages=messages,
                    temperature=0.7,
                    top_p=0.9
                )

                if response.choices and len(response.choices) > 0:
                    llm_output = response.choices[0].message.content.strip()
                    print(f"âœ… ä»»åŠ¡ {task_id}: `{filename}` å¤„ç†å®Œæˆ")
                    progress_bar.update(1)

                    # æ¸…ç†å¹¶ä¿å­˜å½“å‰å­ä»»åŠ¡çš„ç»“æœ
                    cleaned_result = await clean_and_parse_llm_result(llm_output)
                    base_filename = os.path.splitext(filename)[0]  # å»æ‰æ–‡ä»¶æ‰©å±•å
                    output_path = os.path.join(OUTPUT_DIR, f"{base_filename}.json")  # ä¿å­˜ä¸º .json æ–‡ä»¶
                    with open(output_path, "w", encoding="utf-8") as f:
                        json.dump(cleaned_result, f, ensure_ascii=False, indent=4)
                    print(f"ğŸ“ å­ä»»åŠ¡ç»“æœå·²ä¿å­˜: {output_path}")
                    return filename, llm_output

                else:
                    print(f"âš ï¸ ä»»åŠ¡ {task_id}: `{filename}` å“åº”ä¸ºç©º")
                    progress_bar.update(1)
                    return filename, ""

            except json.JSONDecodeError as e:
                retry_count += 1
                print(f'{model} è¿”å›é”™è¯¯ JSON æ ¼å¼æ•°æ®ï¼Œç¬¬ {retry_count} æ¬¡é‡è¯•...')
                time.sleep(1)  # å»¶è¿Ÿ 1 ç§’åé‡è¯•
                break

            except Exception as e:
                retry_count += 1
                print(f"ğŸ”´ ä»»åŠ¡ {task_id}: `{filename}` è°ƒç”¨å¤±è´¥ï¼Œé‡è¯• {retry_count}/{max_retries} æ¬¡: {e}")
                await asyncio.sleep(2)  # ç­‰å¾…ä¸€æ®µæ—¶é—´åå†é‡è¯•
                break

        print(f"âš ï¸ ä»»åŠ¡ {task_id}: `{filename}` å¤„ç†å¤±è´¥")
        progress_bar.update(1)
        return filename, ""

# ========== å¹¶å‘è°ƒç”¨ LLM APIï¼ˆå¹¶ç›‘æ§è¿›åº¦ï¼‰ ========== #
async def call_llm_json_parallel_async(messages_dict, model, semaphore):
    tasks = []
    with tqdm(total=len(messages_dict), desc="è°ƒç”¨ LLM ä¸­...") as progress_bar:
        for task_id, (filename, messages) in enumerate(messages_dict.items()):
            task = asyncio.create_task(call_single_llm_json_async(task_id + 1, filename, messages, model, progress_bar, semaphore))
            tasks.append(task)

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        results = await asyncio.gather(*tasks)

    return results

# ========== ä¸»ç¨‹åº ========== #
async def main():
    prompt_template = read_prompt_template()
    txt_contents = read_and_split_files()

    if not txt_contents:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½• txt æˆ– json æ–‡ä»¶ï¼")
        return

    # åˆ›å»ºå¹¶å‘ä¿¡å·é‡
    semaphore = asyncio.Semaphore(MAX_CONCURRENT_TASKS)

    # å¹¶å‘ç”Ÿæˆ messagesï¼Œå¹¶åŠ å…¥è¿›åº¦ç›‘æ§
    messages_dict = await generate_messages_concurrently(prompt_template, txt_contents, semaphore)

    # å¹¶å‘è°ƒç”¨ LLMï¼ˆç›‘æ§è¿›åº¦ï¼‰
    await call_llm_json_parallel_async(messages_dict, MODEL_NAME, semaphore)

if __name__ == "__main__":
    asyncio.run(main())
